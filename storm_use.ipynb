{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from knowledge_storm import STORMWikiRunnerArguments, STORMWikiRunner, STORMWikiLMConfigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from knowledge_storm.lm import TogetherClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from knowledge_storm.rm import YouRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_configs = STORMWikiLMConfigs()\n",
    "together_kwargs = {\n",
    "    'api_key': os.getenv(\"950355c3c42f5721ecff1331e416362254c0b1e45ed23f43519a856707a161dd\"),\n",
    "    # 'api_key': os.getenv(\"TOGETHER_API_KEY\"),\n",
    "    'temperature': 1.0,\n",
    "    'top_p': 0.9,\n",
    "    \"stop\": ('\\n\\n---',)\n",
    "}\n",
    "# STORM is a LM system so different components can be powered by different models to reach a good balance between cost and quality.\n",
    "# For a good practice, choose a cheaper/faster model for `conv_simulator_lm` which is used to split queries, synthesize answers in the conversation.\n",
    "# Choose a more powerful model for `article_gen_lm` to generate verifiable text with citations.\n",
    "llama_8B = TogetherClient(model='meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo', max_tokens=500, **together_kwargs)\n",
    "# llama_70B = TogetherClient(model='gpt-4o', max_tokens=3000, **together_kwargs)\n",
    "lm_configs.set_conv_simulator_lm(llama_8B)\n",
    "# lm_configs.set_question_asker_lm(llama_3B)\n",
    "# lm_configs.set_outline_gen_lm(llama_70B)\n",
    "# lm_configs.set_article_gen_lm(llama_70B)\n",
    "# lm_configs.set_article_polish_lm(llama_70B)\n",
    "# # Check out the STORMWikiRunnerArguments class for more configurations.\n",
    "# engine_args = STORMWikiRunnerArguments(...)\n",
    "# rm = YouRM(ydc_api_key=os.getenv('YDC_API_KEY'), k=engine_args.search_top_k)\n",
    "# runner = STORMWikiRunner(engine_args, lm_configs, rm)\n",
    "\n",
    "# TogetherClient(dspy.HFModel):\n",
    "#     \"\"\"A wrapper class for dspy.Together.\"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         model,\n",
    "#         api_key: Optional[str] = None,\n",
    "#         apply_tokenizer_chat_template=False,\n",
    "#         hf_tokenizer_name=None,\n",
    "#         model_type: Literal[\"chat\", \"text\"] = \"chat\",\n",
    "#         **kwargs,\n",
    "#     ):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6 (main, Oct  2 2023, 20:46:17) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
